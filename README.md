# FlightMind

**An aviation-native language model trained on data generated by the system it powers.**

FlightMind is a transformer LLM built from scratch to serve as the language reasoning engine for [AIDA](https://github.com/kushkoirala/AIDA) (Autonomous Intelligent Decision Architecture) -- an autonomous flight system that flies cross-country missions in a Cessna 172 with natural language pilot interaction.

What makes FlightMind novel is its **closed-loop training paradigm**: the autonomous system generates the data that trains the language model that powers the autonomous system. Each iteration produces higher-quality flights, which produce higher-quality training data, which produces a better model.

<p align="center">
  <img src="docs/figures/closed_loop_architecture.png" alt="Closed-Loop Training Paradigm" width="700"/>
</p>

## Why Not Just Fine-Tune Llama?

We could. But building from scratch gives us:

1. **Full control over architecture** -- optimized for the specific task (command parsing, not general chat)
2. **Size efficiency** -- a 1-2B aviation-specialist model outperforms an 8B generalist for structured flight commands
3. **Inference speed** -- 3-5x faster than Llama 8B on the same hardware, critical for real-time flight control
4. **Educational value** -- every layer, every design choice is documented and understood
5. **No license restrictions** -- fully owned, no Meta/commercial license concerns

The model architecture is inspired by Andrej Karpathy's [nanochat](https://github.com/karpathy/nanochat) approach: a depth-parameterized transformer where a single integer controls the entire model size.

## Project Status

**Phase: d24 cloud pretraining in progress**

| Milestone | Status | Details |
|-----------|--------|---------|
| Data collection | Done | 192M aviation tokens from 8 sources |
| Data cleaning pipeline | Done | 157K docs cleaned and validated |
| Custom BPE tokenizer | Done | 32K vocab trained on aviation + general text |
| Model architecture | Done | Transformer with RoPE, SwiGLU, RMSNorm, Flash Attention |
| Training infrastructure | Done | AdamW, cosine LR, grad accumulation, DDP, mixed precision |
| Evaluation suite | Done | Perplexity, sample generation, domain probing |
| d8 pretraining (50M) | Done | Perplexity 8.12, 71 tok/s generation on RTX 4060 |
| AIDA fine-tuning data pipeline | Done | 303K instruction pairs from real flights |
| **d24 cloud pretraining (956M)** | **In Progress** | **4x H100 SXM on Vast.ai, ~$506 est.** |
| d32 cloud pretraining (2.2B) | Planned | 4-8x H100 SXM, ~$1,240 est. |
| Instruction fine-tuning | Planned | LoRA on 303K AIDA instruction pairs |
| AIDA integration | Planned | Drop-in replacement for Llama 8B |

## The Closed-Loop Data Pipeline

<p align="center">
  <img src="docs/figures/data_composition.png" alt="Data Composition" width="700"/>
</p>

### Phase 1: Aviation Corpus (Pretraining)

General aviation knowledge from public sources -- the model learns to "speak aviation."

| Source | Documents | Tokens | Content |
|--------|-----------|--------|---------|
| NTSB accident reports | 92,000+ | 135M | Investigation narratives, probable cause |
| METAR weather observations | 22,000+ | 33M | Real weather data from US airports |
| HuggingFace aviation dataset | 15,000+ | 15M | Curated aviation text |
| Wikipedia aviation articles | 9,600+ | 20M | Aircraft, airports, aviation concepts |
| 14 CFR regulations | 900+ | 2.7M | Federal aviation regulations |
| FAA handbooks | 7 | 2.7M | PHAK, AFH, IFH, weight & balance |
| OpenAP aircraft performance | 7,400+ | 3M | Aircraft specs, performance models |
| **Total** | **157,118** | **~192M** | |

### Phase 2: AIDA Flight Data (Fine-tuning)

Training data generated directly from AIDA's autonomous flights -- the model learns AIDA's specific command language.

| Source | Examples | Content |
|--------|----------|---------|
| AIDA intent observations | 244,228 | Real command/action pairs from 148K sim observations |
| Synthetic command variations | 55,000 | 9 categories: heading, altitude, speed, landing, weather, emergency |
| XC flight telemetry | 3,945 | Status narration + phase transitions from 40 recorded flights |
| **Total** | **303,173** | **~106 MB** |

Each fine-tuning example is an instruction-response pair:

```
<|system|>You are FlightMind, an aviation AI copilot for AIDA.<|end|>
<|user|>PHASE: cruise | ALT: 4500ft | SPD: 110kt | HDG: 270
turn to heading three six zero<|end|>
<|assistant|>{"action": "heading", "value": 360}
Roger, turning to heading 360.<|end|>
```

The model learns both **structured output** (JSON for the flight controller) and **pilot-style acknowledgement** (natural language for the human) in a single inference.

### Phase 3: Deploy Back to AIDA

The trained FlightMind model replaces Llama 8B as AIDA's language engine:

```
Before: Pilot command  -->  Llama 8B (5GB, ~0.5s)  -->  Flight controller
After:  Pilot command  -->  FlightMind (1-4GB, ~0.1s)  -->  Flight controller
```

Same WebSocket interface (port 8766), same FlightCommand output format, faster inference, aviation-native understanding.

## Architecture

### Depth Parameterization

The entire model is controlled by a single integer, `depth`, from which all architectural dimensions are derived. This enables seamless scaling from a 13M-parameter smoke test to a 2.2B-parameter production model by changing one number.

<p align="center">
  <img src="docs/figures/architecture_scaling.png" alt="Architecture Scaling" width="700"/>
</p>

| Component | Choice | Why |
|-----------|--------|-----|
| Normalization | RMSNorm (pre-norm) | Faster than LayerNorm, stable training |
| Positional encoding | RoPE | Extrapolates to longer sequences, no learned params |
| Activation | SwiGLU | Better quality than GELU/ReLU, standard for modern LLMs |
| Attention | Flash Attention | 2-4x faster, memory efficient |
| Weight tying | Embedding = LM head | Saves parameters, acts as regularizer |
| Optimizer | AdamW (beta2=0.95) | Standard for LLM pretraining |
| LR schedule | Linear warmup + cosine decay | Prevents early instability, smooth convergence |
| Precision | bfloat16 mixed precision | 2x throughput on GPU |

See [ARCHITECTURE.md](model/ARCHITECTURE.md) and [TRAINING.md](train/TRAINING.md) for detailed educational documentation on every design decision.

### Training Pipeline

```
Aviation corpus (192M tokens, local)  +  FineWeb-EDU (~1.3T tokens, streamed)
  --> BPE tokenize (32K vocab)
  --> Pack into 2048-token sequences (30% aviation / 70% general)
  --> Multi-GPU DDP pretrain on 4x H100 (AdamW, cosine LR, bfloat16)
  --> Evaluate (perplexity + generation)

AIDA flight data (303K instruction pairs)
  --> Instruction fine-tuning (LoRA, chat format)
  --> Evaluate on command parsing accuracy
  --> Deploy to AIDA
```

### Inference on AIDA Hardware

FlightMind is designed to run inference on the same RTX 4060 that powers AIDA's flight simulation:

| Model | Params | Inference VRAM | Tokens/sec | Capability |
|-------|--------|---------------|------------|------------|
| d20 | 566M | ~1.5 GB | ~200+ | Structured commands |
| d24 | 956M | ~2.5 GB | ~120+ | + reasoning |
| d32 | 2.2B | ~5.5 GB | ~60+ | + complex queries |
| Llama 8B (current) | 8B | ~5 GB (Q4) | ~30 | Overkill for task |

## Training Results

### d8 (50M params) -- Complete

Trained on RTX 4060 with 109M aviation tokens. 5,000 steps, 10.3 hours, 1.31B tokens processed.

<p align="center">
  <img src="docs/figures/training_loss.png" alt="Training Loss Curve" width="700"/>
</p>

| Step | Train Loss | Val Loss | Perplexity | Notes |
|------|-----------|----------|------------|-------|
| 0 | 10.45 | -- | 34,600 | Random initialization |
| 250 | 3.53 | 3.42 | 30.4 | Learning basic patterns |
| 500 | 2.51 | 3.01 | 20.2 | Coherent text emerging |
| 1,000 | 2.08 | 2.42 | 11.2 | Beating GPT-2 (124M) on domain text |
| 2,000 | 1.88 | 2.07 | 7.9 | Strong aviation vocabulary |
| **3,500** | **1.75** | **1.95** | **7.0** | **Best checkpoint (early stopping)** |
| 5,000 | 1.60 | 2.19 | 8.9 | Overfitting (val loss rising) |

**Final evaluation on best checkpoint (step 3,500):**
- **Perplexity: 8.12** (100-batch evaluation, 1.08M tokens)
- **Generation speed: 71.4 tok/s** on RTX 4060
- Loss range across batches: [1.07, 2.81] -- low on structured data (METARs), higher on narrative (NTSB reports)

For reference, GPT-2 (124M params) achieves perplexity ~29 on general web text. Our 50M model achieves **8.12** on aviation text -- less than half the parameters, a quarter the perplexity. Domain specialization works.

<p align="center">
  <img src="docs/figures/training_dashboard.png" alt="Training Metrics Dashboard" width="700"/>
</p>

### d24 (956M params) -- In Progress

Scaling to production size on Vast.ai cloud GPUs. The d24 model is 19x larger than d8, requiring multi-GPU training with DistributedDataParallel (DDP) across NVLink-connected H100s.

**Why d24?** At 956M parameters, FlightMind-d24 sits in the sweet spot between capability and inference cost. It's large enough for reasoning-level tasks (command disambiguation, multi-step flight planning) while staying under 2.5 GB VRAM for inference -- comfortably within the RTX 4060's budget alongside AIDA's other GPU workloads.

#### Cloud Training Specifications

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| **Hardware** | 4x NVIDIA H100 80GB SXM (NVLink) | NVLink gives ~900 GB/s GPU-to-GPU bandwidth for fast gradient sync |
| **Provider** | Vast.ai (on-demand) | Best $/GPU-hr for short-term training; no long-term commitment |
| **Instance cost** | $6.40/hr ($1.60/GPU/hr) | Competitive on-demand H100 pricing |
| **Parallelism** | DDP (data parallel, 4-way) | Model fits on one GPU (15 GB); DDP gives near-linear throughput scaling |
| **Precision** | bfloat16 mixed precision | 2x throughput on H100, no loss scaling needed |
| **Effective batch** | 524K tokens/step | 8 micro-batches x 8 grad accum steps x 4 GPUs x 2047 tokens |
| **Learning rate** | 6e-4 peak, cosine decay to 6e-5 | Standard for ~1B models; 500-step linear warmup |
| **Total steps** | 95,000 | ~50B token-presentations (Chinchilla-optimal for 956M params) |
| **Data mix** | 30% aviation (local) / 70% FineWeb-EDU (streamed) | Aviation tokens packed locally; general data streamed from HuggingFace |
| **Throughput** | ~175K tok/s | ~3.0 sec/step across 4 GPUs |
| **Wall-clock time** | ~79 hours (~3.3 days) | Including FineWeb stream init (~15s) |
| **Estimated cost** | ~$506 | 79h x $6.40/hr |

#### Data Strategy: Streaming FineWeb-EDU

With only 108M tokens stored locally (aviation corpus), training for 95K steps at 524K tokens/step would repeat the data ~460 times -- far too much repetition. Instead, we stream the general-domain portion (70% of each batch) directly from HuggingFace's FineWeb-EDU dataset (~1.3T tokens of high-quality educational web text).

Each micro-batch is split: **2 sequences from local aviation data + 6 sequences from FineWeb-EDU** = 8 total. The streamer tokenizes and packs documents on-the-fly with the same BPE tokenizer used for the aviation corpus. In DDP mode, each GPU gets a different shard of the stream so no data is duplicated.

#### Multi-GPU Setup: What We Learned

- **NCCL NVLS incompatibility**: Vast.ai Docker containers don't support NCCL's NVLink Sharp (NVLS) multicast feature. DDP hangs silently at `DDP(model, device_ids=[...])` during model initialization. Fix: `NCCL_NVLS_ENABLE=0`. NVLink P2P still works -- only the NVLS optimization is disabled, with no measurable performance impact for sub-2B models.
- **Gradient accumulation with `no_sync()`**: During the 8 micro-steps of gradient accumulation, only the final micro-step triggers the NCCL all-reduce. This reduces inter-GPU communication by 8x.
- **Rank-aware I/O**: Only GPU rank 0 logs, evaluates, and saves checkpoints. Other ranks wait at `dist.barrier()` synchronization points.

#### Early Training Progress

| Step | Train Loss | Throughput | Notes |
|------|-----------|------------|-------|
| 0 | 10.69 | 164K tok/s | Random initialization |
| 10 | 8.61 | 172K tok/s | Rapid initial learning |
| 20 | 8.06 | 175K tok/s | Throughput stabilizing |
| 30 | 7.81 | 175K tok/s | Steady convergence |
| 40 | 7.14 | 175K tok/s | Still in LR warmup |
| 50 | 6.80 | 175K tok/s | Loss dropping well |

## Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Collect and clean data
python scripts/collect/collect_all.py
python scripts/process/clean_all.py

# Build tokenizer
python tokenizer/train_tokenizer.py

# Prepare training data
python train/dataloader.py

# Train (smoke test on CPU)
python train/pretrain.py --depth 4 --device cpu --max-steps 100

# Train (single GPU)
python train/pretrain.py --depth 8 --device cuda --batch-size 4

# Train (multi-GPU with DDP)
torchrun --nproc_per_node=4 train/pretrain.py --depth 24 --batch-size 8 --fineweb

# Evaluate
python train/evaluate.py --checkpoint checkpoints/best.pt --all

# Generate AIDA fine-tuning data
python data/collectors/convert_intent_observations.py
python data/collectors/generate_synthetic_commands.py
python data/collectors/convert_xc_telemetry.py
```

## Project Structure

```
FlightMind/
|-- README.md                          # This file
|-- requirements.txt                   # Python dependencies
|-- config.yaml                        # Project configuration
|
|-- model/                             # Model architecture
|   |-- __init__.py
|   |-- config.py                      # Depth-parameterized config
|   |-- flightmind.py                  # Transformer implementation
|   +-- ARCHITECTURE.md                # Architecture documentation
|
|-- train/                             # Training pipeline
|   |-- pretrain.py                    # Pretraining loop
|   |-- finetune.py                    # LoRA instruction fine-tuning
|   |-- dataloader.py                  # Data loading + tokenization
|   |-- evaluate.py                    # Evaluation + generation
|   +-- TRAINING.md                    # Training documentation
|
|-- tokenizer/                         # Custom BPE tokenizer
|   |-- train_tokenizer.py             # Tokenizer training script
|   +-- tokenizer.json                 # Trained tokenizer (gitignored)
|
|-- data/
|   |-- raw/                           # Downloaded source data (gitignored)
|   |-- cleaned/                       # Processed text (gitignored)
|   |-- tokenized/                     # Binary token arrays (gitignored)
|   |-- finetune/                      # AIDA fine-tuning data
|   +-- collectors/                    # Data conversion scripts
|       |-- convert_intent_observations.py
|       |-- generate_synthetic_commands.py
|       +-- convert_xc_telemetry.py
|
|-- scripts/
|   |-- collect/                       # Data collection scripts
|   +-- process/                       # Data cleaning scripts
|
|-- docs/
|   |-- cloud_requirements_d24.md      # d24 cloud training specs
|   |-- cloud_requirements_d32.md      # d32 cloud training specs
|   |-- vastai_setup_guide.md          # Vast.ai deployment guide
|   |-- generate_figures.py            # Chart/figure generation script
|   +-- figures/                       # Publication-quality figures
|
|-- checkpoints/                       # Model checkpoints (gitignored)
+-- run_train.ps1                      # PowerShell training launcher
```

## Hardware

| Role | Hardware | Notes |
|------|----------|-------|
| Data processing + d8 training | Dell 7920 (2x Xeon Gold 5118, 64GB, RTX 4060) | Local development |
| d24/d32 pretraining | 4x H100 80GB SXM (NVLink) on Vast.ai | Cloud GPU rental, DDP |
| AIDA inference | Same Dell 7920 + RTX 4060 | FlightMind serves AIDA |

## Relationship to AIDA

FlightMind is the **language brain** of AIDA. AIDA handles:
- Classical flight control (PID autopilot, FSM phase management)
- GPU-accelerated 6-DOF physics (CUDA, 500M+ steps/sec)
- Bayesian intent inference (von Mises/Gaussian posteriors)
- Safety enforcement (Control Barrier Functions)
- Residual RL policy (PPO-trained neural corrections)

FlightMind handles:
- Natural language command parsing ("turn heading 270" -> structured action)
- Flight status narration (telemetry -> human-readable reports)
- Weather interpretation (METAR/TAF decoding)
- Aviation knowledge queries (V-speeds, procedures, regulations)
- Pilot-style communication (ATC phraseology, acknowledgements)

Together they form a complete autonomous flight system where the pilot speaks naturally and the aircraft responds with both precise control actions and clear communication.

## Documentation

This is an educational project. Every design decision is documented:

- [ARCHITECTURE.md](model/ARCHITECTURE.md) -- Model architecture decisions (RoPE, SwiGLU, RMSNorm, Flash Attention, weight tying)
- [TRAINING.md](train/TRAINING.md) -- Training pipeline (AdamW, LR schedules, gradient accumulation, DDP, mixed precision, scaling laws)
- [Cloud Requirements (d24)](docs/cloud_requirements_d24.md) -- GPU, VRAM, storage, and cost estimates for d24 training
- [Cloud Requirements (d32)](docs/cloud_requirements_d32.md) -- Same for the larger d32 model
- [Vast.ai Setup Guide](docs/vastai_setup_guide.md) -- Step-by-step cloud deployment with NCCL fixes and monitoring

## Acknowledgments

- [Andrej Karpathy](https://github.com/karpathy) -- nanochat, nanoGPT, llm.c (architecture inspiration)
- [HuggingFace](https://huggingface.co) -- tokenizers library, FineWeb-EDU, datasets
- FAA, NTSB, NASA -- public aviation data
- [OpenAP](https://openap.dev) (TU Delft) -- aircraft performance models

## License

MIT
