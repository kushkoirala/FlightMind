# FlightMind

**An aviation-native language model trained on data generated by the system it powers.**

FlightMind is a transformer LLM built from scratch to serve as the language reasoning engine for [AIDA](https://github.com/kushkoirala/AIDA) (Autonomous Intelligent Decision Architecture) -- an autonomous flight system that flies cross-country missions in a Cessna 172 with natural language pilot interaction.

What makes FlightMind novel is its **closed-loop training paradigm**: the autonomous system generates the data that trains the language model that powers the autonomous system. Each iteration produces higher-quality flights, which produce higher-quality training data, which produces a better model.

<p align="center">
  <img src="docs/figures/closed_loop_architecture.png" alt="Closed-Loop Training Paradigm" width="700"/>
</p>

## Why Not Just Fine-Tune Llama?

We could. But building from scratch gives us:

1. **Full control over architecture** -- optimized for the specific task (command parsing, not general chat)
2. **Size efficiency** -- a 1-2B aviation-specialist model outperforms an 8B generalist for structured flight commands
3. **Inference speed** -- 3-5x faster than Llama 8B on the same hardware, critical for real-time flight control
4. **Educational value** -- every layer, every design choice is documented and understood
5. **No license restrictions** -- fully owned, no Meta/commercial license concerns

The model architecture is inspired by Andrej Karpathy's [nanochat](https://github.com/karpathy/nanochat) approach: a depth-parameterized transformer where a single integer controls the entire model size.

## Project Status

**Phase: d8 pretraining complete -- scaling to d24**

| Milestone | Status | Details |
|-----------|--------|---------|
| Data collection | Done | 192M aviation tokens from 8 sources |
| Data cleaning pipeline | Done | 157K docs cleaned and validated |
| Custom BPE tokenizer | Done | 32K vocab trained on aviation + general text |
| Model architecture | Done | Transformer with RoPE, SwiGLU, RMSNorm, Flash Attention |
| Training infrastructure | Done | AdamW, cosine LR, gradient accumulation, mixed precision |
| Evaluation suite | Done | Perplexity, sample generation, domain probing |
| **d8 pretraining (50M)** | **Done** | **Perplexity 8.12, 71 tok/s generation** |
| AIDA fine-tuning data pipeline | Done | 303K instruction pairs from real flights |
| d24/d32 cloud pretraining | Planned | A100 GPU, ~$50-150 |
| Instruction fine-tuning | Planned | LoRA on 303K AIDA instruction pairs |
| AIDA integration | Planned | Drop-in replacement for Llama 8B |

## The Closed-Loop Data Pipeline

<p align="center">
  <img src="docs/figures/data_composition.png" alt="Data Composition" width="700"/>
</p>

### Phase 1: Aviation Corpus (Pretraining)

General aviation knowledge from public sources -- the model learns to "speak aviation."

| Source | Documents | Tokens | Content |
|--------|-----------|--------|---------|
| NTSB accident reports | 92,000+ | 135M | Investigation narratives, probable cause |
| METAR weather observations | 22,000+ | 33M | Real weather data from US airports |
| HuggingFace aviation dataset | 15,000+ | 15M | Curated aviation text |
| Wikipedia aviation articles | 9,600+ | 20M | Aircraft, airports, aviation concepts |
| 14 CFR regulations | 900+ | 2.7M | Federal aviation regulations |
| FAA handbooks | 7 | 2.7M | PHAK, AFH, IFH, weight & balance |
| OpenAP aircraft performance | 7,400+ | 3M | Aircraft specs, performance models |
| **Total** | **157,118** | **~192M** | |

### Phase 2: AIDA Flight Data (Fine-tuning)

Training data generated directly from AIDA's autonomous flights -- the model learns AIDA's specific command language.

| Source | Examples | Content |
|--------|----------|---------|
| AIDA intent observations | 244,228 | Real command/action pairs from 148K sim observations |
| Synthetic command variations | 55,000 | 9 categories: heading, altitude, speed, landing, weather, emergency |
| XC flight telemetry | 3,945 | Status narration + phase transitions from 40 recorded flights |
| **Total** | **303,173** | **~106 MB** |

Each fine-tuning example is an instruction-response pair:

```
<|system|>You are FlightMind, an aviation AI copilot for AIDA.<|end|>
<|user|>PHASE: cruise | ALT: 4500ft | SPD: 110kt | HDG: 270
turn to heading three six zero<|end|>
<|assistant|>{"action": "heading", "value": 360}
Roger, turning to heading 360.<|end|>
```

The model learns both **structured output** (JSON for the flight controller) and **pilot-style acknowledgement** (natural language for the human) in a single inference.

### Phase 3: Deploy Back to AIDA

The trained FlightMind model replaces Llama 8B as AIDA's language engine:

```
Before: Pilot command  -->  Llama 8B (5GB, ~0.5s)  -->  Flight controller
After:  Pilot command  -->  FlightMind (1-4GB, ~0.1s)  -->  Flight controller
```

Same WebSocket interface (port 8766), same FlightCommand output format, faster inference, aviation-native understanding.

## Architecture

### Depth Parameterization

The entire model is controlled by a single integer, `depth`, from which all architectural dimensions are derived. This enables seamless scaling from a 13M-parameter smoke test to a 2.2B-parameter production model by changing one number.

<p align="center">
  <img src="docs/figures/architecture_scaling.png" alt="Architecture Scaling" width="700"/>
</p>

| Component | Choice | Why |
|-----------|--------|-----|
| Normalization | RMSNorm (pre-norm) | Faster than LayerNorm, stable training |
| Positional encoding | RoPE | Extrapolates to longer sequences, no learned params |
| Activation | SwiGLU | Better quality than GELU/ReLU, standard for modern LLMs |
| Attention | Flash Attention | 2-4x faster, memory efficient |
| Weight tying | Embedding = LM head | Saves parameters, acts as regularizer |
| Optimizer | AdamW (beta2=0.95) | Standard for LLM pretraining |
| LR schedule | Linear warmup + cosine decay | Prevents early instability, smooth convergence |
| Precision | bfloat16 mixed precision | 2x throughput on GPU |

See [ARCHITECTURE.md](model/ARCHITECTURE.md) and [TRAINING.md](train/TRAINING.md) for detailed educational documentation on every design decision.

### Training Pipeline

```
Aviation corpus (192M tokens)
  --> BPE tokenize (32K vocab)
  --> Pack into 2048-token sequences
  --> Pretrain with next-token prediction (AdamW, cosine LR)
  --> Evaluate (perplexity + generation)

AIDA flight data (303K instruction pairs)
  --> Instruction fine-tuning (chat format)
  --> Evaluate on command parsing accuracy
  --> Deploy to AIDA
```

### Inference on AIDA Hardware

FlightMind is designed to run inference on the same RTX 4060 that powers AIDA's flight simulation:

| Model | Params | Inference VRAM | Tokens/sec | Capability |
|-------|--------|---------------|------------|------------|
| d20 | 566M | ~1.5 GB | ~200+ | Structured commands |
| d24 | 956M | ~2.5 GB | ~120+ | + reasoning |
| d32 | 2.2B | ~5.5 GB | ~60+ | + complex queries |
| Llama 8B (current) | 8B | ~5 GB (Q4) | ~30 | Overkill for task |

## Training Results (d8, complete)

FlightMind-d8 (50M params) trained on RTX 4060 with 109M aviation tokens. 5,000 steps, 10.3 hours, 1.31B tokens processed.

<p align="center">
  <img src="docs/figures/training_loss.png" alt="Training Loss Curve" width="700"/>
</p>

| Step | Train Loss | Val Loss | Perplexity | Notes |
|------|-----------|----------|------------|-------|
| 0 | 10.45 | -- | 34,600 | Random initialization |
| 250 | 3.53 | 3.42 | 30.4 | Learning basic patterns |
| 500 | 2.51 | 3.01 | 20.2 | Coherent text emerging |
| 1,000 | 2.08 | 2.42 | 11.2 | Beating GPT-2 (124M) on domain text |
| 2,000 | 1.88 | 2.07 | 7.9 | Strong aviation vocabulary |
| **3,500** | **1.75** | **1.95** | **7.0** | **Best checkpoint (early stopping)** |
| 5,000 | 1.60 | 2.19 | 8.9 | Overfitting (val loss rising) |

**Final evaluation on best checkpoint (step 3,500):**
- **Perplexity: 8.12** (100-batch evaluation, 1.08M tokens)
- **Generation speed: 71.4 tok/s** on RTX 4060
- Loss range across batches: [1.07, 2.81] -- low on structured data (METARs), higher on narrative (NTSB reports)

For reference, GPT-2 (124M params) achieves perplexity ~29 on general web text. Our 50M model achieves **8.12** on aviation text -- less than half the parameters, a quarter the perplexity. Domain specialization works.

<p align="center">
  <img src="docs/figures/training_dashboard.png" alt="Training Metrics Dashboard" width="700"/>
</p>

## Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Collect and clean data
python scripts/collect/collect_all.py
python scripts/process/clean_all.py

# Build tokenizer
python tokenizer/train_tokenizer.py

# Prepare training data
python train/dataloader.py

# Train (smoke test on CPU)
python train/pretrain.py --depth 4 --device cpu --max-steps 100

# Train (GPU)
python train/pretrain.py --depth 8 --device cuda --batch-size 4

# Evaluate
python train/evaluate.py --checkpoint checkpoints/best.pt --all

# Generate AIDA fine-tuning data
python data/collectors/convert_intent_observations.py
python data/collectors/generate_synthetic_commands.py
python data/collectors/convert_xc_telemetry.py
```

## Project Structure

```
FlightMind/
|-- README.md                          # This file
|-- requirements.txt                   # Python dependencies
|-- config.yaml                        # Project configuration
|
|-- model/                             # Model architecture
|   |-- __init__.py
|   |-- config.py                      # Depth-parameterized config
|   |-- flightmind.py                  # Transformer implementation
|   +-- ARCHITECTURE.md                # Architecture documentation
|
|-- train/                             # Training pipeline
|   |-- pretrain.py                    # Pretraining loop
|   |-- finetune.py                    # LoRA instruction fine-tuning
|   |-- dataloader.py                  # Data loading + tokenization
|   |-- evaluate.py                    # Evaluation + generation
|   +-- TRAINING.md                    # Training documentation
|
|-- tokenizer/                         # Custom BPE tokenizer
|   |-- train_tokenizer.py             # Tokenizer training script
|   +-- tokenizer.json                 # Trained tokenizer (gitignored)
|
|-- data/
|   |-- raw/                           # Downloaded source data (gitignored)
|   |-- cleaned/                       # Processed text (gitignored)
|   |-- tokenized/                     # Binary token arrays (gitignored)
|   |-- finetune/                      # AIDA fine-tuning data
|   +-- collectors/                    # Data conversion scripts
|       |-- convert_intent_observations.py
|       |-- generate_synthetic_commands.py
|       +-- convert_xc_telemetry.py
|
|-- scripts/
|   |-- collect/                       # Data collection scripts
|   +-- process/                       # Data cleaning scripts
|
|-- docs/
|   |-- generate_figures.py            # Chart/figure generation script
|   +-- figures/                       # Publication-quality figures
|
|-- checkpoints/                       # Model checkpoints (gitignored)
+-- run_train.ps1                      # PowerShell training launcher
```

## Hardware

| Role | Hardware | Notes |
|------|----------|-------|
| Data processing + training | Dell 7920 (2x Xeon Gold 5118, 64GB, RTX 4060) | Local development |
| Large model training | Cloud A100 80GB | d24/d32 pretraining |
| AIDA inference | Same Dell 7920 + RTX 4060 | FlightMind serves AIDA |

## Relationship to AIDA

FlightMind is the **language brain** of AIDA. AIDA handles:
- Classical flight control (PID autopilot, FSM phase management)
- GPU-accelerated 6-DOF physics (CUDA, 500M+ steps/sec)
- Bayesian intent inference (von Mises/Gaussian posteriors)
- Safety enforcement (Control Barrier Functions)
- Residual RL policy (PPO-trained neural corrections)

FlightMind handles:
- Natural language command parsing ("turn heading 270" -> structured action)
- Flight status narration (telemetry -> human-readable reports)
- Weather interpretation (METAR/TAF decoding)
- Aviation knowledge queries (V-speeds, procedures, regulations)
- Pilot-style communication (ATC phraseology, acknowledgements)

Together they form a complete autonomous flight system where the pilot speaks naturally and the aircraft responds with both precise control actions and clear communication.

## Documentation

This is an educational project. Every design decision is documented:

- [ARCHITECTURE.md](model/ARCHITECTURE.md) -- Model architecture decisions (RoPE, SwiGLU, RMSNorm, Flash Attention, weight tying)
- [TRAINING.md](train/TRAINING.md) -- Training pipeline (AdamW, LR schedules, gradient accumulation, mixed precision, scaling laws)

## Acknowledgments

- [Andrej Karpathy](https://github.com/karpathy) -- nanochat, nanoGPT, llm.c (architecture inspiration)
- [HuggingFace](https://huggingface.co) -- tokenizers library, FineWeb-EDU, datasets
- FAA, NTSB, NASA -- public aviation data
- [OpenAP](https://openap.dev) (TU Delft) -- aircraft performance models

## License

MIT
